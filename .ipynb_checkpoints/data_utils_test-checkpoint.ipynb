{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "import data_utils_orig # our data preprocessing functions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dev data files\n",
    "en_fname = \"./data/dev.en\"\n",
    "vi_fname = \"./data/dev.vi\"\n",
    "\n",
    "# create vocabulary\n",
    "# Create vocabulary file (if it does not exist yet) from data file.\n",
    "data_utils_orig.create_vocabulary(\"./data/dev_vocab.en\",\"./data/dev.en\",100)\n",
    "data_utils_orig.create_vocabulary(\"./data/dev_vocab.vi\",\"./data/dev.vi\",100)\n",
    "\n",
    "# get vocab pairs\n",
    "#Initialize vocabulary from file.\n",
    "# Returns a pair: the vocabulary (a dictionary mapping string to integers), and\n",
    "# the reversed vocabulary (a list, which reverses the vocabulary mapping).\n",
    "en_vocab_pairs = data_utils_orig.initialize_vocabulary(\"./data/dev_vocab.en\")\n",
    "vi_vocam_pairs = data_utils_orig.initialize_vocabulary(\"./data/dev_vocab.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('all', 42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab_pairs[0].iteritems().next()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sentence_to_token_ids(sentence, vocabulary,tokenizer=None, normalize_digits=True):\n",
    "  \"\"\"Convert a string to list of integers representing token-ids.\n",
    "\n",
    "  For example, a sentence \"I have a dog\" may become tokenized into\n",
    "  [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n",
    "  \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_token_id_files(sentencepath,tokenidpath,vocabpath):\n",
    "  from tensorflow.python.platform import gfile\n",
    "  if not gfile.Exists(tokenidpath):\n",
    "    print(\"Creating token id file %s from sentence file %s\" % (tokenidpath, sentencepath))\n",
    "    with gfile.GFile(sentencepath, mode=\"rb\") as f:\n",
    "      counter = 0\n",
    "      for line in f:\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  processing line %d\" % counter)\n",
    "        line = tf.compat.as_bytes(line)\n",
    "      thislinetokenids = data_utils_orig.sentence_to_token_ids(line,vocabpath)\n",
    "      with gfile.GFile(tokenidpath, mode=\"wb\") as tokenid_file:\n",
    "        for w in vocab_list:\n",
    "          vocab_file.write(w + b\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating token id file ./data/def_tokenids.en from sentence file ./data/dev.en\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-aac05ae0bed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_token_id_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/dev.en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./data/def_tokenids.en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./data/dev_vocab.en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcreate_token_id_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/dev.vi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./data/def_tokenids.vi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"./data/dev_vocab.vi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-96e3221f767f>\u001b[0m in \u001b[0;36mcreate_token_id_files\u001b[0;34m(sentencepath, tokenidpath, vocabpath)\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  processing line %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mthislinetokenids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenidpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtokenid_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/1A_MOOCs/deeplearning_practice/SequenceToSequence/SequenceToSequence/data_utils_orig.pyc\u001b[0m in \u001b[0;36msentence_to_token_ids\u001b[0;34m(sentence, vocabulary, tokenizer, normalize_digits)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNK_ID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;31m# Normalize digits by 0 before looking words up in the vocabulary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DIGIT_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNK_ID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "create_token_id_files(\"./data/dev.en\",\"./data/def_tokenids.en\",\"./data/dev_vocab.en\")\n",
    "create_token_id_files(\"./data/dev.vi\",\"./data/def_tokenids.vi\",\"./data/dev_vocab.vi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname1 = \"./data/dev_vocab.en\"\n",
    "fname2 = \"./data/dev_vocab.vi\"\n",
    "\n",
    "\n",
    "\n",
    "# with open(fname1) as f1:\n",
    "#     content1 = f1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_data(source_path, target_path, max_size=None):\n",
    "  \"\"\"Read data from source and target files and put into buckets.\n",
    "\n",
    "  Args:\n",
    "    source_path: path to the files with token-ids for the source language.\n",
    "    target_path: path to the file with token-ids for the target language;\n",
    "      it must be aligned with the source file: n-th line contains the desired\n",
    "      output for n-th line from the source_path.\n",
    "    max_size: maximum number of lines to read, all other will be ignored;\n",
    "      if 0 or None, data files will be read completely (no limit).\n",
    "\n",
    "  Returns:\n",
    "    data_set: a list of length len(_buckets); data_set[n] contains a list of\n",
    "      (source, target) pairs read from the provided data files that fit\n",
    "      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n",
    "      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n",
    "  \"\"\"\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.from\" % FLAGS.from_vocab_size)\n",
    "    fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.to\" % FLAGS.to_vocab_size)\n",
    "    en_vocab, _ = ata_utils.initialize_vocabulary(en_vocab_path)d\n",
    "    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "    # Decode from standard input.\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()\n",
    "    while sentence:\n",
    "      # Get token-ids for the input sentence.\n",
    "      token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "      bucket_id = len(_buckets) - 1\n",
    "      for i, bucket in enumerate(_buckets):\n",
    "        if bucket[0] >= len(token_ids):\n",
    "          bucket_id = i\n",
    "          break\n",
    "      else:\n",
    "        logging.warning(\"Sentence truncated: %s\", sentence)\n",
    "\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "      # Get output logits for the sentence.\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "      # If there is an EOS symbol in outputs, cut them at that point.\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "      # Print out French sentence corresponding to outputs.\n",
    "      print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Read data into buckets and compute their sizes.\n",
    "    print (\"Reading development and training data (limit: %d).\"\n",
    "           % FLAGS.max_train_data_size)\n",
    "    dev_set = read_data(from_dev, to_dev)\n",
    "    train_set = read_data(from_train, to_train, FLAGS.max_train_data_size)\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          train_set, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
