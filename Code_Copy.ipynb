{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataUtils():\n",
    "\n",
    "    def __init__(self, from_train_file='data/train.en',\n",
    "                 from_vocab_file='data/vocab.en',\n",
    "                 to_train_file='data/train.vi',\n",
    "                 to_vocab_file='data/vocab.vi',\n",
    "                 from_dev_file='data/tst2012.en',\n",
    "                 to_dev_file='data/tst2012.vi',\n",
    "                 from_test_file='data/tst2013.en',\n",
    "                 to_test_file='data/tst2013.vi'):\n",
    "\n",
    "        #self._PAD = b\"_PAD\"\n",
    "        #self._GO = b\"_GO\"\n",
    "        #self._EOS = b\"_EOS\"\n",
    "        #self._UNK = b\"_UNK\"\n",
    "        #self._START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "        self.PAD_ID = 0\n",
    "        self.GO_ID = 1\n",
    "        self.EOS_ID = 2\n",
    "        self.UNK_ID = 3\n",
    "\n",
    "        self.from_train_file   = from_train_file\n",
    "        self.source_vocab_file = from_vocab_file\n",
    "        self.to_train_file     = to_train_file\n",
    "        self.to_vocab_file     = to_vocab_file\n",
    "        self.from_dev_file     = from_dev_file\n",
    "        self.to_dev_file       = to_dev_file\n",
    "        self.from_test_file    = from_test_file\n",
    "        self.to_test_file      = to_test_file\n",
    "\n",
    "        vocab, rev_vocab = self.initialize_vocabulary(from_train_file)\n",
    "        self.en_vocab_from_train = vocab\n",
    "        self.en_rev_vocab_from_train = rev_vocab\n",
    "\n",
    "        vocab, rev_vocab = self.initialize_vocabulary(to_train_file)\n",
    "        self.en_vocab_to_train = vocab\n",
    "        self.en_vocab_to_train = rev_vocab\n",
    "\n",
    "        vocab, rev_vocab = self.initialize_vocabulary(from_dev_file)\n",
    "        self.en_vocab_from_dev = vocab\n",
    "        self.en_rev_vocab_from_dev = rev_vocab\n",
    "\n",
    "        vocab, rev_vocab = self.initialize_vocabulary(to_dev_file)\n",
    "        self.en_vocab_to_dev = vocab\n",
    "        self.en_vocab_to_dev = rev_vocab\n",
    "\n",
    "        vocab, rev_vocab = self.initialize_vocabulary(from_test_file)\n",
    "        self.en_vocab_from_dev = vocab\n",
    "        self.en_rev_vocab_from_dev = rev_vocab\n",
    "\n",
    "        vocab, rev_vocab = self.initialize_vocabulary(to_test_file)\n",
    "        self.en_vocab_to_test = vocab\n",
    "        self.en_vocab_to_test = rev_vocab\n",
    "\n",
    "\n",
    "    def initialize_vocabulary(self, vocabulary_path):\n",
    "        if gfile.Exists(vocabulary_path):\n",
    "            rev_vocab = []\n",
    "            with gfile.GFile(vocabulary_path, \"rb\") as f:\n",
    "                rev_vocab.extend(f.readlines())\n",
    "            rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n",
    "            vocab = dict([x,y] for (y,x) in enumerate(rev_vocab))\n",
    "            return vocab, rev_vocab\n",
    "        else:\n",
    "            raise ValueError(\"Vocabulary file %s not found\", vocabulary_path)\n",
    "\n",
    "    def basic_tokenizer(sentence):\n",
    "        \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "        words = []\n",
    "        for space_separated_fragment in sentence.strip().split():\n",
    "            words.extend(self._WORD_SPLIT.split(space_separated_fragment))\n",
    "        return [w for w in words if w]\n",
    "\n",
    "    def sentence_to_token_ids(sentence, vocabulary_path, tokenizer=None, normalize_digits=True):\n",
    "        if tokenizer:\n",
    "            words = tokenizer(sentence)\n",
    "        else:\n",
    "            vocabulary, _ = self.initialize_vocabulary(vocabulary_path)\n",
    "            words = self.basic_tokenizer(sentence)\n",
    "            if not normalize_digits:\n",
    "                return [vocabulary.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "            # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "            return [vocabulary.get(self._DIGIT_RE.sub(b\"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "    def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "        if not gfile.Exists(target_path):\n",
    "            print(\"Tokenizing data in %s\" % data_path)\n",
    "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "        with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
    "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "                counter = 0\n",
    "                for line in data_file:\n",
    "                    counter += 1\n",
    "                    if counter % 100000 == 0:\n",
    "                        print(\"  tokenizing line %d\" % counter)\n",
    "                    token_ids = sentence_to_token_ids(tf.compat.as_bytes(line), vocab,\n",
    "                                                tokenizer, normalize_digits)\n",
    "                tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
    "\n",
    "    def prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path, from_vocabulary_size,\n",
    "                 to_vocabulary_size, tokenizer=None):\n",
    "\n",
    "        # Create token ids for the training data.\n",
    "        to_train_ids_path = to_train_path + (\".ids%d\" % to_vocabulary_size)\n",
    "        from_train_ids_path = from_train_path + (\".ids%d\" % from_vocabulary_size)\n",
    "        data_to_token_ids(to_train_path, to_train_ids_path, to_vocab_path, tokenizer)\n",
    "        data_to_token_ids(from_train_path, from_train_ids_path, from_vocab_path, tokenizer)\n",
    "\n",
    "        # Create token ids for the development data.\n",
    "        to_dev_ids_path = to_dev_path + (\".ids%d\" % to_vocabulary_size)\n",
    "        from_dev_ids_path = from_dev_path + (\".ids%d\" % from_vocabulary_size)\n",
    "        data_to_token_ids(to_dev_path, to_dev_ids_path, to_vocab_path, tokenizer)\n",
    "        data_to_token_ids(from_dev_path, from_dev_ids_path, from_vocab_path, tokenizer)\n",
    "\n",
    "        return (from_train_ids_path, to_train_ids_path,\n",
    "              from_dev_ids_path, to_dev_ids_path,\n",
    "              from_vocab_path, to_vocab_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    du = DataUtils()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from DataUtils import DataUtils\n",
    "from Config import Config\n",
    "from attn_cell import attn_cell, _linear\n",
    "\n",
    "class Seq2SeqModel(LanguageModel):\n",
    "\n",
    "    def load_data(self):\n",
    "        dataset = [[] for _ in self.config.buckets]\n",
    "\n",
    "        from_train_file = self.config.from_train_file\n",
    "        to_train_file = self.config.to_train_file\n",
    "\n",
    "        max_size = self.config.max_size\n",
    "\n",
    "        with tf.gfile.GFile(from_train_file, \"r\") as from_file:\n",
    "            with tf.gfile.GFile(to_train_file, \"r\") as to_file:\n",
    "                from_line, to_line = from_file.readline(), to_file.readline()\n",
    "                while from_line and to_line and (not max_size or count < max_size):\n",
    "                    count = 0\n",
    "                    if count % 5000 == 0:\n",
    "                        print(\"Line: \", count)\n",
    "                    from_ids = [int(x) for x in from_line.split()]\n",
    "                    to_ids = [int(x) for x in to_line.split()]\n",
    "                    to_ids.append(self.config.EOS_ID)\n",
    "\n",
    "                    for bucket_id, (from_size, to_size) in enumerate(self.config.buckets):\n",
    "                        data_set[bucket_id].append([from_ids, to_ids])\n",
    "                    from_line, to_line = from_line.readline(), to_line.readline()\n",
    "        return data_set\n",
    "\n",
    "    def create_feed_dict(self):\n",
    "        feed_dict = {}\n",
    "        encoder_inputs = self.bucketize_encoding_layer()\n",
    "        feed_dict['encoder_inputs'] = encoder_inputs\n",
    "\n",
    "        decoder_inputs, to_weights, targets = self.bucketize_decoding_layer()\n",
    "\n",
    "        feed_dict['decoder_inputs'] = decoder_inputs\n",
    "        feed_dict['to_weights'] = to_weights\n",
    "        feed_dict['targets'] = targets\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def bucketize_encoding_layer():\n",
    "        encoder_inputs = []\n",
    "\n",
    "        for i in xrange(self.config.buckets[-1][0]):\n",
    "            encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                name=\"encoder{0}\".format(i)))\n",
    "        return encoder_inputs\n",
    "\n",
    "    def bucketize_decoding_layer():\n",
    "        decoder_inputs = []\n",
    "        to_weights = []\n",
    "\n",
    "        for i in xrange(self.config.buckets[-1][1]+1):\n",
    "            decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                name=\"decoder{0}\".format(i)))\n",
    "            to_weights.append(tf.placeholder(tf.float32, shape=[None],\n",
    "                                name=\"weight{0}\".format(i)))\n",
    "        targets = [decoder_inputs[i+1] for i in xrange(len(decoder_inputs))]\n",
    "\n",
    "        return (decoder_inputs, to_weights, targets)\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.en_input_placeholder = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.de_input_placeholder = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "    def add_projection(self):\n",
    "        num_samples = self.config.num_samples\n",
    "        if num_samples > 0 and num_samples < self.config.to_vocab_size:\n",
    "            self.w_t = tf.get_variable(\"W\", [self.config.to_vocab_size, self.config.size], dtype=self.config.dtype)\n",
    "            self.w = tf.transpose(w_t)\n",
    "            self.b = tf.get_variable(\"b\", [self.config.to_vocab_size], dtype=self.config.dtype)\n",
    "            return (w,b)\n",
    "\n",
    "    # Embedding and attention function\n",
    "    def add_embedding(self):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        inputs: input placeholder in shape [batch_size, input_size]\n",
    "        output:\n",
    "        embedded: embedded input in shape[batch_size*rnn_hidden, time_step_size]\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.variable_scope(\"embedding\") as scope:\n",
    "                L = tf.get_variable(\"L\",[self.config.from_vocab_size, self.config.encode_hidden_size], initializer = self.config.initializer)\n",
    "                embeds = tf.nn.embedding_lookup(L, self.feed_dict['encoder_inputs'])\n",
    "                embedded = [tf.squeeze(x) for x in tf.split(embeds, [tf.ones([self.config.encode_num_steps], tf.int32)], axis=1)]\n",
    "        return embedded\n",
    "\n",
    "    def add_decode_embedding(self):\n",
    "        with tf.variable_scope(\"decode_embedding\") as decode_scope:\n",
    "            L = tf.get_variable(\"L\", [self.config.to_vocab_size, self.decode_hidden_size], initializer=self.config.initializer)\n",
    "            embeds = tf.nn.embedding_lookup(L, self.feed_dict['decoder_inputs'])\n",
    "            embedded = [tf.squeeze(x) for x in tf.split(embeds, [tf.ones([self.config.decode_num_steps], tf.int32)], axis=1)]\n",
    "        return embedded\n",
    "\n",
    "\n",
    "    def LSTM_cell(self):\n",
    "        self.cell = tf.contrib.rnn.BasicLSTMCell(self.config.encode_hidden_size)\n",
    "        self.decoder_cell = tf.contrib.rnn.attn_cell(self.config.encode_hidden_size,en_states)\n",
    "        self.encoder_cell = tf.contrib.rnn.BasicLSTMCell(self.config.encode_hidden_size)\n",
    "\n",
    "    def encoder_layer(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: embedded encoder inputs\n",
    "        outputs: a tuple of (outputs, states)\n",
    "        \"\"\"\n",
    "        initial_state = (tf.zeros([self.config.batch_size, self.config.encode_hidden_size]), tf.zeros([self.config.batch_size, self.config.encode_hidden_size]))\n",
    "        state = initial_state\n",
    "        cell = self.cell\n",
    "        outputs = []\n",
    "        states = []\n",
    "\n",
    "        for i in xrange(self.config.encode_num_steps):\n",
    "            output, state = cell(inputs, state)\n",
    "            inputs = output\n",
    "            outputs.append(output)\n",
    "            states.append(state)\n",
    "\n",
    "        return (outputs, states)\n",
    "\n",
    "    def decoder_layer(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: embedded encoder inputs\n",
    "        outputs: a tuple of (outputs, states)\n",
    "        \"\"\"\n",
    "        initial_state = (tf.zeros([self.config.batch_size, self.config.decode_hidden_size]), tf.zeros([self.config.batch_size, self.config.decode_hidden_size]))\n",
    "        state = initial_state\n",
    "        cell = self.decoder_cell\n",
    "        outputs = []\n",
    "        states = []\n",
    "\n",
    "        for i in xrange(self.config.decode_num_steps):\n",
    "            output, state = cell(inputs, state)\n",
    "            inputs = output\n",
    "            outputs.append(output)\n",
    "            states.append(state)\n",
    "\n",
    "        return (outputs, states)\n",
    "\n",
    "    # Loss function\n",
    "    def add_loss_op(self, inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1,1])\n",
    "        w_t = tf.cast(self.w_t, self.config.dtype)\n",
    "        b = tf.cast(self.b, self.config.dtype)\n",
    "        inputs = tf.cast(inputs, self.config.dtype)\n",
    "\n",
    "        softmax_loss = tf.cast(\n",
    "            tf.nn.sampled_softmax_loss(\n",
    "                weights=w_t,\n",
    "                biases=b,\n",
    "                labels=labels,\n",
    "                inputs=inputs,\n",
    "                num_sampled=num_samples,\n",
    "                num_classes=self.config.to_vocab_size),\n",
    "                self.config.dtype\n",
    "            )\n",
    "        return softmax_loss\n",
    "\n",
    "\n",
    "    def add_embeddings(self, encoder_inputs, decoder_inputs, cell, is_decode):\n",
    "        embeddings = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "            encoder_inputs, decoder_inputs, cell,\n",
    "            num_encoder_symbols=self.config.from_vocab_size,\n",
    "            num_decoder_symbols=self.config.to_vocab_size,\n",
    "            embedding_size=self.config.size,\n",
    "            output_projection=projection,\n",
    "            feed_previous=is_decode,\n",
    "            dtype=self.config.dtype\n",
    "        )\n",
    "\n",
    "    def add_to_model_with_buckets(self, encoder_inputs, decoder_inputs, targets, weights):\n",
    "        all_inputs = encoder_inputs + decoder_inputs + targets + weights\n",
    "        losses = []\n",
    "        outputs = []\n",
    "        with tf.variable_scope(\"en_de_model\") as model_scope:\n",
    "            for i, bucket in enumerate(self.config.buckets):\n",
    "                buckets_outputs, _ = lambda x, y: self.add_embeddings(x, y, cell, projection, do_decode)\n",
    "                outputs.append(buckets_outputs)\n",
    "                losses.append(sequence_loss(\n",
    "                                outputs[-1],\n",
    "                                targets[:buckets[1]],\n",
    "                                weights[:buckets[1]],\n",
    "                                softmax_loss_function=self.add_loss_op\n",
    "                                ))\n",
    "        return outputs, losses\n",
    "\n",
    "\n",
    "    # Add model\n",
    "    def add_model(self, cell, projection):\n",
    "        encoder_inputs = self.feed_dict['encoder_inputs']\n",
    "        decoder_inputs = self.feed_dict['decoder_inputs']\n",
    "        to_weights     = self.feed_dict['to_weights']\n",
    "        targets        = self.feed_dict['targets']\n",
    "\n",
    "        self.outputs, self.losses = self.add_to_model_with_buckets(\n",
    "                encoder_inputs, decoder_inputs, targets, weights\n",
    "        )\n",
    "\n",
    "        if self.config.forward_only:\n",
    "            if projection is not None:\n",
    "                for b xrange(len(self.config.buckets)):\n",
    "                    self.outputs[b] = tf.matmul(x, projection[0] + projection[1] for x in self.outputs[b])\n",
    "        else:\n",
    "            params = tf.trainable_variables()\n",
    "            self.gradient_norms = []\n",
    "            self.updates = []\n",
    "\n",
    "            self.lr = tf.Variable(float(self.config.lr),\n",
    "                                trainable=False, dtype=self.config.dtype)\n",
    "            self.lr_decay_op = self.lr.assign(self.lr*self.config.learning_rate_decay)\n",
    "            self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "\n",
    "            for b in xrange(len(self.config.buckets)):\n",
    "                gradients = tf.gradients(self.losses[b], params)\n",
    "                clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                self.config.max_gradient_norm)\n",
    "                self.gradient_norms.append(norm)\n",
    "                self.updates.append(optimier.apply_gradients(\n",
    "                    zip(clipped_gradients, params),  global_step=self.global_step\n",
    "                ))\n",
    "        self.saver = tf.train.saver(tf.global_variables())\n",
    "\n",
    "    def step(self, session, encoder_inputs, decoder_inputs, targets, to_weights, b):\n",
    "        input_feed = {}\n",
    "        for i in xrange(len(encoder_inputs)):\n",
    "            name = self.feed_dict['encoder_inputs'][i].name\n",
    "            input_feed[name] = encoder_inputs[i]\n",
    "\n",
    "        for i in xrange(len(decoder_inputs)):\n",
    "            de_name = self.feed_dict['decoder_inputs'][i].name\n",
    "            to_name = self.feed_dict['to_weights'][i].name\n",
    "            input_feed[de_name] = decoder_inputs[i]\n",
    "            input_feed[to_name] = to_weights[i]\n",
    "\n",
    "        to_last = self.feed_dict['decoder_inputs'][len(decoder_inputs)].name\n",
    "        input_feed[to_last] = np.zeros([self.config.batch_size], dtype=np.int32)\n",
    "\n",
    "        if self.config.forward_only:\n",
    "            output_feed = self.losses[b]\n",
    "            for i in xrange(len(decoder_inputs)):\n",
    "                output_feed.append(self.outputs[b][i])\n",
    "        else:\n",
    "            output_feed = [self.updates[b], self.gradient_norms[b], self.losses[b]]\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if self.config.forward_only:\n",
    "            return None, outputs[0], outputs[1]\n",
    "        else:\n",
    "            return outputs[1], outputs[2], None\n",
    "\n",
    "    def __init__(self, do_decode=False):\n",
    "        self.config = Config\n",
    "\n",
    "        data_set = self.load_data()\n",
    "        self.add_placeholders()\n",
    "\n",
    "        self.feed_dict = self.create_feed_dict()\n",
    "\n",
    "        output_projection = self.add_projection()\n",
    "\n",
    "        # Create the internal multi-layer cell\n",
    "        if self.config.num_layers == 1:\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(self.config.size)\n",
    "        else:\n",
    "            single_cell = tf.contrib.rnn.BasicLSTMCell(self.config.size)\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([single_cell for _ in self.config.num_layers])\n",
    "\n",
    "\n",
    "        self.add_model(cell, output_projection, do_decode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
